[
["index.html", "Analysis Report Preface", " Analysis Report Niklas Johannes 2020-09-11 Preface On this site, I document all steps, from data processing to final analyses, for the project [project title.] "],
["setting-up.html", "1 Setting-up", " 1 Setting-up In this section, I load all libraries and define all custom functions that we need for data processing and analysis. Note I use the pacman package for loading libraries. # pacman makes it easier to load and install packages if (!requireNamespace(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) library(pacman) # load packages p_load( tidyverse, here ) # set seed set.seed(42) "],
["data-processing.html", "2 Data processing 2.1 Read waves 2-6 2.2 Read wave 1 2.3 Data cleaning 2.4 Response rates and missing values", " 2 Data processing In this section I’ll process the raw data so that I can use them for analysis in the next section. Currently, I’m allowed to share the raw data, which is why they’re not on the Github repo yet. 2.1 Read waves 2-6 We’ll start with loading the data for waves 2 to 6. We start here instead of wave 1 because participant identification isn’t super straightforward. A participant doesn’t get a single unique identifier that’s stable across all waves. Instead, participants start with an ID in wave 1 (called transaction_idOld in the wave 2 to 6 data) and get a new unique ID for wave 2 and so on. I believe originally the researchers wanted to go with a single ID, but changed their mind. Look at the data table below (2.1), where I made up two participants to recreate the data structure. We see that the first participant has an transaction ID (transaction_idOld) for the first wave. The same participant then has that same old transaction ID for wave two, plus a new transaction ID for wave 2 (transaction_id). Here’s where it gets a bit complicated: That transaction ID for wave 2 is repeated on wave 3, but with a different name (transaction_idW2). Next to that new variable, is the unique ID for that wave (transaction_idW3). After that, each new wave has the ID of the previous wave plus a new ID for the current wave. ## Parsed with column specification: ## cols( ## id = col_double(), ## transaction_old = col_double(), ## transaction_id = col_double(), ## transaction_idW2 = col_double(), ## transaction_idW3 = col_double(), ## transaction_idW4 = col_double(), ## transaction_idW5 = col_double(), ## transaction_idW6 = col_double(), ## v1 = col_double(), ## v2 = col_double(), ## week = col_double() ## ) Table 2.1: Example of how ID variables look id transaction_old transaction_id transaction_idW2 transaction_idW3 transaction_idW4 transaction_idW5 transaction_idW6 v1 v2 week 1 978344791 NA NA NA NA NA NA 2 23 1 1 978344791 987910814 NA NA NA NA NA 3 2342 2 1 NA NA 987910814 994717737 NA NA NA 41 533 3 1 NA NA NA 994717737 998317051 NA NA 213 534 4 1 NA NA NA NA 998317051 1005222579 NA 4 4125 5 1 NA NA NA NA NA 1005222579 1018946937 523 3123 6 2 978344792 NA NA NA NA NA NA 2 23 1 2 978344792 987910815 NA NA NA NA NA 3 2342 2 2 NA NA 987910815 NA NA NA NA 41 533 3 2 NA NA NA NA NA NA NA 213 534 4 2 NA NA NA NA NA NA NA 4 4125 5 2 NA NA NA NA NA NA NA 523 3123 6 Unfortunately, the id column from the example isn’t included in the raw data, so we’ll have to recreate participant IDs that are constant across waves. I solved this problem with a loop, which is definitely not the most elegant way to go about this, but I think it gets the job done. First, lets load the data. waves_2_to_6 &lt;- read_csv( here(&quot;data&quot;, &quot;waves_2_to_6.csv&quot;), guess_max = 2e4 ) ## Warning: Duplicated column names deduplicated: &#39;transaction_id&#39; =&gt; &#39;transaction_id_1&#39; [522] ## Parsed with column specification: ## cols( ## .default = col_double(), ## uuid = col_character(), ## date = col_character(), ## markers = col_character(), ## PS1a = col_logical(), ## PS1b = col_logical(), ## PS1c = col_logical(), ## noanswerPS1c_r96 = col_logical(), ## A2 = col_logical(), ## B1r1 = col_logical(), ## B1r2 = col_logical(), ## B1r3 = col_logical(), ## B1r4 = col_logical(), ## B1r5 = col_logical(), ## B1r6 = col_logical(), ## B1r7 = col_logical(), ## B1r8 = col_logical(), ## B1r9 = col_logical(), ## B1r10 = col_logical(), ## B1r11 = col_logical(), ## B1r12 = col_logical() ## # ... with 162 more columns ## ) ## See spec(...) for full column specifications. Next, the logic behind assigning the constant IDs. We’ll start at the ID of wave 2, find the ID of wave 3 that’s on the same line as the ID of wave 2 and store the ID for wave 3. We repeat this procedure for all waves and in the end assign an ID to those rows that have the IDs we extracted on any of the ID variables. The steps, concretely: We store all IDs (so that’s the IDs of wave 2) in a vector (excluding NAs). Initiate an ID counter (i.e., the participant number we assign later). We iterate over each wave 2 ID. If participants did participate in a wave, we find the wave 3 ID that is on the same row as the wave 2 that we’re currently looking for. We now look for the wave 3 ID and store the wave 4 ID that’s on the same line. We repeat that until we’re at the last ID and have all IDs for that participant stored in a vector. Then a row gets assigned the participant ID (to the existing, but empty id variable) if any of the wave ID variables have a match in the extracted IDs. On my machine, the below code takes about 30 seconds. # the unique wave 2 IDs ids &lt;- waves_2_to_6 %&gt;% pull(transaction_id) %&gt;% na.omit # the participant number we assign later id_counter &lt;- 1 # assign the correct variable type to the empty ID variable that&#39;s already in the data where we&#39;ll assign participant numbers at the end of the loop waves_2_to_6$id &lt;- NA_real_ for (an_id in ids) { # transaction ids that belong to that one participant id_to_match &lt;- c() # add the initial id id_to_match &lt;- c(id_to_match, an_id) # assign that id as temporary ID for which we should match (id_to_match only has one entry so far) temp_id &lt;- id_to_match # at this point, participants might have not participated in the next wave, so they might have an empty cell in # transaction_id2. so we&#39;ll embed everything that follows in their separate if statement if (!is_empty(temp_id)) { # find the transcation_id3 that&#39;s on the same line as transaction_id2 and store it temp_id &lt;- waves_2_to_6 %&gt;% filter(transaction_idW2 == an_id) %&gt;% pull(transaction_idW3) id_to_match &lt;- na.omit(c(id_to_match, temp_id)) # so that NA doesn&#39;t become one of the IDs } if (!is_empty(temp_id)){ # then the same for transaction_id4, but this time we compare to the temporary id we extracted above # also, we only select those rows where the next ID isn&#39;t NA (because there&#39;re two matches for the temp_id) temp_id &lt;- waves_2_to_6 %&gt;% filter((transaction_idW3 == temp_id) &amp; !is.na(transaction_idW4)) %&gt;% pull(transaction_idW4) id_to_match &lt;- c(id_to_match, temp_id) } if (!is_empty(temp_id)){ # then the same as above for transaction_id5 temp_id &lt;- waves_2_to_6 %&gt;% filter((transaction_idW4 == temp_id) &amp; !is.na(transaction_idW5)) %&gt;% pull(transaction_idW5) id_to_match &lt;- c(id_to_match, temp_id) } if (!is_empty(temp_id)){ # and last for transaction_id6 temp_id &lt;- waves_2_to_6 %&gt;% filter((transaction_idW5 == temp_id) &amp; !is.na(transaction_idW6)) %&gt;% pull(transaction_idW6) id_to_match &lt;- c(id_to_match, temp_id) } # then we assign a participant number, such that a participant gets the number for each row where any of their transaction_ids are part of the temporary ids waves_2_to_6 &lt;- waves_2_to_6 %&gt;% mutate( id = case_when( transaction_id %in% id_to_match ~ id_counter, transaction_idW2 %in% id_to_match ~ id_counter, transaction_idW3 %in% id_to_match ~ id_counter, transaction_idW4 %in% id_to_match ~ id_counter, transaction_idW5 %in% id_to_match ~ id_counter, transaction_idW6 %in% id_to_match ~ id_counter, TRUE ~ id ) ) # increase the participant number id_counter &lt;- id_counter + 1 } Before we we assign correct variable types, name factor levels etc. we’ll load wave 1. This way, we can merge the two files first before doing all of the data cleaning, variable selection, and variable renaming. 2.2 Read wave 1 Let’s load the file for wave 1. transcaction_id is the unique identifier for each participant. However, that variables is called transaction_idOld in waves 2 to 6, which is why we name it the same here (see above). Also, dWeekMerged is the wave identifier that’s used in waves 2 to 6, so we’ll include that manually here. wave1 &lt;- read_csv( here(&quot;data&quot;, &quot;wave1.csv&quot;), guess_max = 2e4 ) %&gt;% rename(transaction_idOld = transaction_id) %&gt;% mutate(dWeekMerged = 1) ## Parsed with column specification: ## cols( ## .default = col_double(), ## uuid = col_character(), ## C1x7ar98oe = col_character(), ## C1x9ar98oe = col_character(), ## C2x7ar10oe = col_character(), ## C2x9ar12oe = col_character(), ## C3x7ar10oe = col_character(), ## C5x7ar12oe = col_character(), ## C7ax7ar10oe = col_character(), ## C7bx7ar9oe = col_character(), ## C7cx7ar11oe = col_character(), ## C3x9ar12oe = col_character() ## ) ## See spec(...) for full column specifications. There are many variables we don’t care about, but before we make the variable selection, I’ll first merge the two files. Here’s another complication: If we look at the surveys and how variables are named there, it looks like there are different labels for the same variables in wave 1 and waves 2 to 6. For example, in wave 1, variables providing the estimated time with a medium had the appendix ai and those asking about self-estimated increases or decreases in frequency had the appendix aii. For waves 2 to 6, the surveys say that this is reversed, such that estimated time with a medium has the appendix aii and self-estimated increase or decrease has the appendix ai. The same goes for variable names: For example, the audiobooks section in wave 1 begins with C7c, but with C6c in waves 2 to 6. In the actual data, however, variable names are constant (see the codebook). For example, audiobooks are C7c in both data sets despite what the survey files say. The variable labels are the same across all waves (I manually checked), which makes merging easier. My merging strategy is as follows: I assign the id we created above to the wave1 data file by merging by transaction_idOld which is available in both data sets. However, that means that those who didn’t participate in wave 2 won’t have an id. So we manually fill those empty cells up - although we’ll probably exclude those who only participant in wave 1 later anyways. I then add the rows of waves_2_to_6 to waves1. I use the bind_rows command, which maintains all variables for which there’s no match and sets them to NA - exactly what we want, because full demographics are only in wave 1 and newer questions only in later waves. Note that from now on I leave the raw wave data untouched and make all changes in a working file. # add the id variable to wave1 by merging by transaction_idOld that&#39;s in both data sets working_file &lt;- left_join( wave1, waves_2_to_6 %&gt;% select(transaction_idOld, id), by = &quot;transaction_idOld&quot; ) # assign an id to those in wave 1 who have an NA in the id column because they didn&#39;t participate in later waves # we can do this easily with coalesce, for which we create a vector that continues from the current max ID until # each participant with missing ID has one working_file &lt;- working_file %&gt;% mutate( id = coalesce( id, max(working_file$id, na.rm = T)+1:nrow(working_file) ) ) # then add the rows of waves 2 to 6 to wave 1 working_file &lt;- bind_rows( working_file, waves_2_to_6 ) 2.3 Data cleaning Now that each participant has a constant identifier and all waves are merged, we can select those variables we actually need. For our purposes, we’ll retain: demographic information variables indicating what media people used in the past week time estimates of those uses self-estimated importance of the medium self-estimated frequency the estimated effects on well-being in wave 6 Note that there are quite a lot of filtering variables. We can’t collapse these to one variable because these questions were multiple choice. working_file &lt;- working_file %&gt;% select( # meta information id, wave = dWeekMerged, # demographics occupation_main_earner = PS1a, main_earner = PS1b, income_main_earner = PS1c, income_own = A7, ethnicity = PS2, internet_use = A2, gender = A4, age = A5, employment = A6x1, living_on_my_own = A6x2r1, living_with_children = A6x2r2, living_with_partner = A6x2r3, living_with_partner_children = A6x2r4, living_with_family = A6x2r5, living_with_parents = A6x2r6, living_with_friends = A6x2r7,, region = A8, covid_work_home_often = A6x4r1, covid_work_home_always = A6x4r2, covid_work_outside = A6x4r3, covid_work_stopped = A6x4r4, covid_home_schooling = A6x4r5, covid_self_isolating = A6x4r6, covid_none = A6x4r99, # filter questions: downloaded downloaded_music = B1r1, downloaded_music_videos = B1r2, downloaded_video_games = B1r3, downloaded_software = B1r4, downloaded_films = B1r5, downloaded_tv = B1r6, downloaded_sports = B1r7, downloaded_video_clips = B1r8, downloaded_ebooks = B1r9, downloaded_magazines = B1r10, downloaded_audiobooks = B1r11, downloaded_images = B1r12, downloaded_none = B1r13, # filter questions: streamed streamed_music = B2r1, streamed_music_videos = B2r2, streamed_video_games = B2r3, streamed_software = B2r4, streamed_films = B2r5, streamed_tv = B2r6, streamed_sports = B2r7, streamed_video_clips = B2r8, streamed_ebooks = B2r9, streamed_magazines = B2r10, streamed_audiobooks = B2r11, streamed_images = B2r12, streamed_none = B2r13, # filter questions: shared shared_music = B3r1, shared_music_videos = B3r2, shared_video_games = B3r3, shared_software = B3r4, shared_films = B3r5, shared_tv = B3r6, shared_sports = B3r7, shared_video_clips = B3r8, shared_ebooks = B3r9, shared_magazines = B3r10, shared_audiobooks = B3r11, shared_images = B3r12, shared_none = B3r13, # filter questions: bought bought_music = B4r1, bought_video_games = B4r2, bought_software = B4r3, bought_films = B4r4, bought_tv = B4r5, bought_books = B4r6, bought_magazines = B4r7, bought_audiobooks = B4r8, bought_none = B4r9, # music music_identity_ = C1x1r1:C1x1r7, music_hours = C1x1aiHoursc1, music_minutes = C1x1aiMinutesc2, music_estimate = C1x1aii, # films films_identity_ = C2x1r1:C2x1r7, films_hours = C2x1aiHoursc1, films_minutes = C2x1aiMinutesc2, films_estimate = C2x1aii, # tv tv_identity_ = C3x1r1:C3x1r7, tv_hours = C3x1aiHoursc1, tv_minutes = C3x1aiMinutesc2, tv_estimate = C3x1aii, # video games games_identity_ = C5x1r1:C5x1r7, games_hours = C5x1aiHoursc1, games_minutes = C5x1aiMinutesc2, games_estimate = C5x1aii, # e-publishing e_publishing_identity_ = C7x1r1:C7x1r7, # (e-)books books_hours = C7ax1aiHoursc1, books_minutes = C7ax1aiMinutesc2, books_estimate = C7ax1aii, # magazines magazines_hours = C7bx1aiHoursc1, magazines_minutes = C7bx1aiMinutesc2, magazines_estimate = C7bx1aii, # audiobooks audiobooks_hours = C7cx1aiHoursc1, audiobooks_minutes = C7cx1aiMinutesc2, audiobooks_estimate = C7cx1aii, # well-being life_satisfaction_ = QD2r1:QD2r2, well_being_ = QD2r3:QD2r4 ) Alright, now to the tedious part: transforming all variables to the correct type and assigning informative factor level labels. Note that id now is numeric which can be misleading, which is why I add pp_ (for participant) before the ID number. working_file &lt;- working_file %&gt;% # turn non-numeric variables into factors mutate( across( c( id, occupation_main_earner:gender, employment:covid_none ), as.factor ) ) %&gt;% # purely cosmetic: arrange by id arrange(id) %&gt;% # give proper labels to demographics mutate( occupation_main_earner = fct_recode( occupation_main_earner, &quot;Higher managerial/professional/administrative&quot; = &quot;1&quot;, &quot;Intermediate managerial/professional/administrative&quot; = &quot;2&quot;, &quot;Supervisory or clerical/junior managerial/ professional/ administrative&quot; = &quot;3&quot;, &quot;Skilled manual worker&quot; = &quot;4&quot;, &quot;Semi-skilled or unskilled work&quot; = &quot;5&quot;, &quot;In full time education&quot; = &quot;6&quot;, &quot;Unemployed&quot; = &quot;7&quot;, &quot;Prefer not to say&quot; = &quot;8&quot; ), across( c(income_main_earner, income_own), ~ fct_recode( .x, &quot;Less than £10,000&quot; = &quot;1&quot;, &quot;£10,000 - £19,999&quot; = &quot;2&quot;, &quot;£20,000 - £29,999&quot; = &quot;3&quot;, &quot;£30,000 - £39,999&quot; = &quot;4&quot;, &quot;£40,000 - £49,999&quot; = &quot;5&quot;, &quot;£50,000 - £59,999&quot; = &quot;6&quot;, &quot;£60,000 - £69,999&quot; = &quot;7&quot;, &quot;£70,000 - £79,999&quot; = &quot;8&quot;, &quot;£80,000 - £89,999&quot; = &quot;9&quot;, &quot;£90,000 - £99,999&quot; = &quot;10&quot;, &quot;£100,000 - £109,999&quot; = &quot;11&quot;, &quot;£110,000 - £119,999&quot; = &quot;12&quot;, &quot;£120,000 - £129,999&quot; = &quot;13&quot;, &quot;£130,000 - £139,999&quot; = &quot;14&quot;, &quot;£140,000 - £149,999&quot; = &quot;15&quot;, &quot;£150,000 - £159,999&quot; = &quot;16&quot;, &quot;£160,000 - £169,999&quot; = &quot;17&quot;, &quot;£170,000 - £179,999&quot; = &quot;18&quot;, &quot;£180,000 - £189,999&quot; = &quot;19&quot;, &quot;£190,000 - £199,999&quot; = &quot;20&quot;, &quot;£200,000 - £209,999&quot; = &quot;21&quot;, &quot;£210,000 - £219,999&quot; = &quot;22&quot;, &quot;£220,000 - £229,999&quot; = &quot;23&quot;, &quot;£230,000 - £239,999&quot; = &quot;24&quot;, &quot;£240,000 - £249,999&quot; = &quot;25&quot;, &quot;£250,000 or more&quot; = &quot;26&quot; ) ), ethnicity = fct_recode( ethnicity, &quot;White British&quot; = &quot;1&quot;, &quot;White European&quot; = &quot;2&quot;, &quot;White - Other&quot; = &quot;3&quot;, &quot;Indian&quot; = &quot;4&quot;, &quot;Pakistani&quot; = &quot;5&quot;, &quot;Bangladeshi&quot; = &quot;6&quot;, &quot;Chinese&quot; = &quot;7&quot;, &quot;Japanese&quot; = &quot;8&quot;, &quot;Any other Asian/Asian British Origin&quot; = &quot;9&quot;, &quot;African&quot; = &quot;10&quot;, &quot;Caribbean&quot; = &quot;11&quot;, &quot;Any other Black/African/Caribbean British origin&quot; = &quot;12&quot;, &quot;Arab&quot; = &quot;13&quot;, &quot;Any other ethnic origin&quot; = &quot;14&quot;, &quot;White and Black Caribbean&quot; = &quot;15&quot;, &quot;White and Black African&quot; = &quot;16&quot;, &quot;White and Asian&quot; = &quot;17&quot;, &quot;Any other Mixed/Multiple ethnic origins&quot; = &quot;18&quot;, &quot;Prefer not to say&quot; = &quot;19&quot; ), internet_use = fct_recode( internet_use, &quot;Several times a day&quot; = &quot;1&quot;, &quot;Once a day&quot; = &quot;2&quot;, &quot;5-6 days a week&quot; = &quot;3&quot;, &quot;2-4 days a week&quot; = &quot;4&quot;, &quot;Once a week&quot; = &quot;5&quot;, &quot;2-3 times a month&quot; = &quot;6&quot;, &quot;Once a month&quot; = &quot;7&quot;, &quot;Less often&quot; = &quot;8&quot;, &quot;Don&#39;t know/Not sure&quot; = &quot;9&quot; ), gender = fct_recode( gender, &quot;Male&quot; = &quot;1&quot;, &quot;Female&quot; = &quot;2&quot;, &quot;Other&quot; = &quot;3&quot; ), employment = fct_recode( employment, &quot;Employed full time&quot; = &quot;1&quot;, &quot;Employed part time&quot; = &quot;2&quot;, &quot;Employed but furloughed due to the coronavirus pandemic&quot; = &quot;3&quot;, &quot;Unemployed and looking for work&quot; = &quot;4&quot;, &quot;Unemployed and not looking for work&quot; = &quot;5&quot;, &quot;Self employed&quot; = &quot;6&quot;, &quot;Retired&quot; = &quot;7&quot;, &quot;Student&quot; = &quot;8&quot;, &quot;Rather not say&quot; = &quot;9&quot; ), region = fct_recode( region, &quot;North East&quot; = &quot;1&quot;, &quot;North West&quot; = &quot;2&quot;, &quot;Yorkshire and The Humber&quot; = &quot;3&quot;, &quot;East Midlands&quot; = &quot;4&quot;, &quot;West Midlands&quot; = &quot;5&quot;, &quot;East&quot; = &quot;6&quot;, &quot;London&quot; = &quot;7&quot;, &quot;South East&quot; = &quot;8&quot;, &quot;South West&quot; = &quot;9&quot;, &quot;Wales&quot; = &quot;10&quot;, &quot;Scotland&quot; = &quot;11&quot;, &quot;Northern Ireland&quot; = &quot;12&quot;, &quot;Not in the UK&quot; = &quot;13&quot; ), # assign yes/no to binary variables across( c( starts_with(&quot;living&quot;), starts_with(&quot;covid&quot;), ), ~ fct_recode( .x, &quot;Yes&quot; = &quot;1&quot;, &quot;No&quot; = &quot;0&quot; ) ), main_earner = fct_recode( main_earner, &quot;Yes&quot; = &quot;1&quot;, &quot;No&quot; = &quot;2&quot; ), # add &quot;pp_&quot; prefix to id variable id = as.factor( paste0(&quot;pp_&quot;, id) ) ) ## Warning: Problem with `mutate()` input `occupation_main_earner`. ## i Unknown levels in `f`: 8 ## i Input `occupation_main_earner` is `fct_recode(...)`. ## Warning: Unknown levels in `f`: 8 ## Warning: Problem with `mutate()` input `..2`. ## i Unknown levels in `f`: 18, 20, 22 ## i Input `..2` is `across(...)`. ## Warning: Unknown levels in `f`: 18, 20, 22 ## Warning: Problem with `mutate()` input `..2`. ## i Unknown levels in `f`: 17, 19, 23, 24 ## i Input `..2` is `across(...)`. ## Warning: Unknown levels in `f`: 17, 19, 23, 24 ## Warning: Problem with `mutate()` input `internet_use`. ## i Unknown levels in `f`: 7 ## i Input `internet_use` is `fct_recode(...)`. ## Warning: Unknown levels in `f`: 7 ## Warning: Problem with `mutate()` input `region`. ## i Unknown levels in `f`: 13 ## i Input `region` is `fct_recode(...)`. ## Warning: Unknown levels in `f`: 13 For the first wave, participants only responded to questions about how much they used a medium if they indicated that they had used it in the three months before wave 1. Those variables (e.g. bought_books) are only present in wave 1, but NA in the other waves. Therefore, we create new variables that show whether a person was asked to indicate their use of a medium, so if they answered yes to any of the filter variables at the beginning of the wave 1 survey. At this point, those filter variables are still numeric, so we’ll add them up. If they’re above 0, participants were asked about that medium. If they’re 0, participants hadn’t used any of the media in the three months before wave 1 and weren’t asked questions about them. Note that I keep those filter variables as numeric for processing later. After we’re done with the filter variables, we can also delete the individual ones. Note that I check at the beginning of the code chunk whether any of those filter variables have missing values, but it seems the survey had forced responses here, so we don’t have missings. # check whether any of the filter variables (at wave 1, when they were asked) have missing values working_file %&gt;% filter(wave == 1) %&gt;% summarise( across( c( starts_with(&quot;downloaded&quot;), starts_with(&quot;streamed&quot;), starts_with(&quot;shared&quot;), starts_with(&quot;bought&quot;) ), ~unique(is.na(.x)) ) ) %&gt;% t() %&gt;% knitr::kable() ## Warning in kable_pipe(x = structure(c(&quot;downloaded_music&quot;, &quot;downloaded_music_videos&quot;, : The table should have a header (column names) downloaded_music FALSE downloaded_music_videos FALSE downloaded_video_games FALSE downloaded_software FALSE downloaded_films FALSE downloaded_tv FALSE downloaded_sports FALSE downloaded_video_clips FALSE downloaded_ebooks FALSE downloaded_magazines FALSE downloaded_audiobooks FALSE downloaded_images FALSE downloaded_none FALSE streamed_music FALSE streamed_music_videos FALSE streamed_video_games FALSE streamed_software FALSE streamed_films FALSE streamed_tv FALSE streamed_sports FALSE streamed_video_clips FALSE streamed_ebooks FALSE streamed_magazines FALSE streamed_audiobooks FALSE streamed_images FALSE streamed_none FALSE shared_music FALSE shared_music_videos FALSE shared_video_games FALSE shared_software FALSE shared_films FALSE shared_tv FALSE shared_sports FALSE shared_video_clips FALSE shared_ebooks FALSE shared_magazines FALSE shared_audiobooks FALSE shared_images FALSE shared_none FALSE bought_music FALSE bought_video_games FALSE bought_software FALSE bought_films FALSE bought_tv FALSE bought_books FALSE bought_magazines FALSE bought_audiobooks FALSE bought_none FALSE While we’re at it: All constant wave 1 variables now have NA in the subsequent wave. I’ll set those NAs to the wave 1 value because those are stable demographics that apply to each wave. Note that three demographic questions were asked at each wave, employment status, living siutation, and the consequences of COVID-19. # get filter variables (only present at wave 1) filters &lt;- working_file %&gt;% filter(wave == 1) %&gt;% # the filter per category mutate( filter_music = rowSums( select( ., starts_with(&quot;downloaded_music&quot;), starts_with(&quot;streamed_music&quot;), starts_with(&quot;shared_music&quot;), bought_music ) ), filter_films = rowSums( select( ., downloaded_films, streamed_films, shared_films, bought_films ) ), filter_tv = rowSums( select( ., downloaded_tv, streamed_tv, shared_tv, bought_tv ) ), filter_video_games = rowSums( select( ., downloaded_video_games, streamed_video_games, shared_video_games, bought_video_games ) ), filter_ebooks = rowSums( select( ., downloaded_ebooks, streamed_ebooks, shared_ebooks, bought_books ) ), filter_magazines = rowSums( select( ., downloaded_magazines, streamed_magazines, shared_magazines, bought_magazines ) ), filter_audiobooks = rowSums( select( ., downloaded_audiobooks, streamed_audiobooks, shared_audiobooks, bought_audiobooks ) ) ) %&gt;% # recode depending on whether the sum is zero or not mutate( across( starts_with(&quot;filter&quot;), ~ if_else(.x &gt; 0, 1, 0) ) ) %&gt;% # select variables that are constant across all waves select( id, occupation_main_earner:internet_use, contains(&quot;identity&quot;), starts_with(&quot;filter&quot;) ) # add those filters and constant variables so that they become a constant for each pp, deleting old filter variables working_file &lt;- left_join( working_file %&gt;% select( -(occupation_main_earner:internet_use), -contains(&quot;identity&quot;) ), filters, by = &quot;id&quot; ) %&gt;% select( -starts_with(&quot;downloaded&quot;), -starts_with(&quot;streamed&quot;), -starts_with(&quot;shared&quot;), -starts_with(&quot;bought&quot;), ) %&gt;% # some reordering for purely cosmetic purposes select( id:age, region, ethnicity, employment, main_earner, occupation_main_earner, starts_with(&quot;income&quot;), starts_with(&quot;living&quot;), starts_with(&quot;covid&quot;), internet_use, starts_with(&quot;filter&quot;), everything() ) # remove the temp filters file rm(filters) 2.4 Response rates and missing values In this section, I check response rates, response patterns, and missing values. The data set is quite complicated because of the many filter variables. The survey had forced responses, so there aren’t any missings if someone finished the survey. However, because of the many filters, there’ll still be a large amount of missing values that we need to inspect or recode. I think the following information is most relevant to understand response and missingness patterns: How many participants have completed each wave. How many responses we have per medium per wave. 2.4.1 Participants per wave First, let’s see how many people completed each wave, what percentage of people have completed that exact number of wave, how many participants that have exactly this number of waves, and what percentage of participants have completed at least this wave. completion_table &lt;- working_file %&gt;% count(id) %&gt;% count(n) %&gt;% rename( &quot;Waves&quot; = n, &quot;Participants (only that number of waves)&quot; = nn ) %&gt;% mutate( &quot;Frequency&quot; = round(`Participants (only that number of waves)` / sum(`Participants (only that number of waves)`), digits = 3) * 100, &quot;Participants per wave&quot; = rev(cumsum(rev(`Participants (only that number of waves)`))), &quot;Frequency per wave&quot; = round(`Participants per wave` / sum(`Participants (only that number of waves)`), digits = 3) * 100 ) %&gt;% as_tibble() ## Storing counts in `nn`, as `n` already present in input ## i Use `name = &quot;new_name&quot;` to pick a new name. knitr::kable(completion_table) Waves Participants (only that number of waves) Frequency Participants per wave Frequency per wave 1 1071 27.7 3863 100.0 2 423 11.0 2792 72.3 3 237 6.1 2369 61.3 4 185 4.8 2132 55.2 5 873 22.6 1947 50.4 6 1074 27.8 1074 27.8 2.4.2 Responses per wave and medium Okay, next we inspect how many responses we have per medium per wave. The data set gets complicated here. This is where the filtering requires quite some wrangling: Someone who hasn’t listened to music in the three months before wave 1 will have missing values on all items about music - but only at wave 1. However, at each subsequent wave, participants were asked one of the estimate questions. If they said that they had used a medium less, about the same, or more compared to the previous week, they were then also prompted to provide their use estimates. For example, someone might’ve had a 0 on the filter questions for audio books because they hadn’t listened to audio books the three months before wave 1. For wave 1, then, they didn’t provide the minutes and hours they spent on audio books. In wave 2, they were then asked how their audio book use had changed since the last survey. If they answered anything but the sixth answer option (i.e., they still hadn’t listened to audio books), they were asked to report how many minutes and hours they had used audio books. Here’s why that can be problematic: The participants below hadn’t listened to audio books in the first three months and reports that at wave 1, then says at wave 2 that their audio book use hadn’t changed (i.e., selected 3 on the audiobooks_estimate question). But the participant was still asked about their minutes and hours - which is why they filled in bogus numbers. Minutes and hours were force response as far as I can tell. I wouldn’t take those estimates seriously, because the participant was forced to respond to minutes and hours. We see that in the following waves, they selected the option that they hadn’t listened to audiobooks. working_file %&gt;% filter(id == &quot;pp_7&quot;) %&gt;% filter(filter_audiobooks == 0) %&gt;% select( id, wave, filter_audiobooks, audiobooks_estimate, audiobooks_minutes, audiobooks_hours ) %&gt;% knitr::kable() id wave filter_audiobooks audiobooks_estimate audiobooks_minutes audiobooks_hours pp_7 1 0 NA NA NA pp_7 2 0 3 1 1 pp_7 3 0 6 NA NA pp_7 4 0 6 NA NA pp_7 5 0 6 NA NA Before we turn to those cases where we can tell a participant didn’t want to provide a use estimate, we need to decide what to do with the filter questions in the first wave. For our research question, we’re interested in how variations in amount of use relate to well-being, not what that relation is among users. Therefore, if someone says that they didn’t use a medium in the three months before the first wave, they are saying that they spent zero amount of time engaging with that medium. The filter questions didn’t ask whether a user has a device they can use to engage with those content categories or whether they had any of the media. So the filter question didn’t ask whether participants were able to engage with a category. It asked whether people used a medium. Therefore, it fits our research question to treat those values as zero. Below, I’ll set all time variables at the first wave to zero that were skipped in the survey because participants said they didn’t use the medium in the filter questions. There’s probably a sleek way to write a function that pulls variable names and matches those with the conditional commands in the code, but that’s above my skill level. working_file &lt;- working_file %&gt;% mutate( # music across( c(music_hours, music_minutes), ~ if_else((wave == 1 &amp; filter_music == 0), 0, .x) ), # films across( c(films_hours, films_minutes), ~ if_else((wave == 1 &amp; filter_films == 0), 0, .x) ), # tv across( c(tv_hours, tv_minutes), ~ if_else((wave == 1 &amp; filter_tv == 0), 0, .x) ), # video games across( c(games_hours, games_minutes), ~ if_else((wave == 1 &amp; filter_video_games == 0), 0, .x) ), # ebooks across( c(books_hours, books_minutes), ~ if_else((wave == 1 &amp; filter_ebooks == 0), 0, .x) ), # magazines across( c(magazines_hours, magazines_minutes), ~ if_else((wave == 1 &amp; filter_magazines == 0), 0, .x) ), # music across( c(audiobooks_hours, audiobooks_minutes), ~ if_else((wave == 1 &amp; filter_audiobooks == 0), 0, .x) ) ) Now, when participants said they didn’t use a medium for any wave, in the next wave they’re still presented the estimate question. When they answered anything but that they didn’t use the medium, they were asked to provide minutes and hours estimates. In those hours and minutes survey questions, they were shown what they said in the previous wave. Crucially, they saw their estimate from the previous wave after they had given their estimate. So when I used 0 minutes in the previous week but forget what I said in the previous wave, it’s not unrealistic that I select “a lot less” on the estimate question. In my mind, estimating use in relation to previous time points and giving an absolute estimate of minutes and hours are two separate psychological retrieval processes. Therefore, I don’t have a problem with someone who used 0 minutes of audio books in a wave saying that they used audio books a lot less in the next wave and report to have used them for half an hour. It doesn’t make sense from an objective stand point because 30 minutes is more than 0 minutes, but the estimate question is about participants’ perceived frequency in relation to the past. For that reason, I won’t directly change the 30 minutes to 0, but leave them as is. In other words, I won’t set values to NA by comparing the absolute minutes and hours estimate to the relative frequency estimate. I’ll only make changes in clear cases like the one presented above, where someone clearly didn’t use a medium. I’ll do exclusions in the next section when I look at data quality. For now, let’s deal with those few cases that are as clear-cut as the one presented above. First some house keeping. If participants indicated on one of the estimate questions that they didn’t use that medium in the previous week (only applicable in waves 2 to 6), they’ll receive an NA on their time estimates. So we’ll first set time estimates to 0 if participants say that they didn’t use a medium. working_file &lt;- working_file %&gt;% mutate( # music across( c(music_hours, music_minutes), ~ if_else((wave != 1 &amp; music_estimate == 6), 0, .x) ), # films across( c(films_hours, films_minutes), ~ if_else((wave != 1 &amp; films_estimate == 6), 0, .x) ), # tv across( c(tv_hours, tv_minutes), ~ if_else((wave != 1 &amp; tv_estimate == 6), 0, .x) ), # video games across( c(games_hours, games_minutes), ~ if_else((wave != 1 &amp; games_estimate == 6), 0, .x) ), # ebooks across( c(books_hours, books_minutes), ~ if_else((wave != 1 &amp; books_estimate == 6), 0, .x) ), # magazines across( c(magazines_hours, magazines_minutes), ~ if_else((wave != 1 &amp; magazines_estimate == 6), 0, .x) ), # music across( c(audiobooks_hours, audiobooks_minutes), ~ if_else((wave != 1 &amp; audiobooks_estimate == 6), 0, .x) ) ) Alright, now let’s check, per medium, who only has 6s and 3s on the estimate questions (aka didn’t use and no change to previous week). Those minutes and hours we’ll set to 0 as well. This applies only to those who didn’t use a medium in the first wave. If you used a medium in the first wave and said that your time remained the same, you shouldn’t get a zero. Setting instances of time with a medium within a participant but across waves to 0 requires some serious data wrangling (or my coding game isn’t strong enough…). To be able to assess who only gave 3 or 6 as an estimate for a medium for waves 2 through 6, we first transform the data to long format. In Table X I display how the data look like after transformation. media_long &lt;- # first turn long by filter variables pivot_longer( working_file %&gt;% select( id, wave, contains(&quot;estimate&quot;), contains(&quot;minutes&quot;), contains(&quot;hours&quot;) ), c(-id, -wave), names_to = c(&quot;medium&quot;, &quot;time&quot;), values_to = &quot;value&quot;, names_sep = &quot;_&quot; ) %&gt;% # now the medium and time are mixed up in the same column, so we spread them pivot_wider( ., id:medium, names_from = &quot;time&quot;, values_from = &quot;value&quot; ) knitr::kable( head(media_long, n= 20) ) id wave medium estimate minutes hours pp_1 1 music 2 11 2 pp_1 1 films 2 1 5 pp_1 1 tv 2 1 7 pp_1 1 games NA 0 0 pp_1 1 books 2 1 3 pp_1 1 magazines 2 1 3 pp_1 1 audiobooks NA 0 0 pp_1 2 music 1 13 10 pp_1 2 films 1 13 13 pp_1 2 tv 1 15 17 pp_1 2 games 1 8 7 pp_1 2 books 1 11 14 pp_1 2 magazines 1 9 15 pp_1 2 audiobooks 1 8 6 pp_2 1 music 1 1 5 pp_2 1 films NA 0 0 pp_2 1 tv NA 0 0 pp_2 1 games NA 0 0 pp_2 1 books NA 0 0 pp_2 1 magazines NA 0 0 Next, I need to select only those media which participants didn’t say they used at the first wave. Below, I select those, add a marker for that id by medium combination, and then add that marker to the long data frame. # select id by medium combinations that didn&#39;t give an estimate in the fist wave (aka the only NA entries) markers &lt;- media_long %&gt;% filter(is.na(estimate)) %&gt;% select(id, medium) %&gt;% mutate(selected = TRUE) # add those markers to the long data frame media_long &lt;- left_join( media_long, markers, by = c(&quot;id&quot;, &quot;medium&quot;) ) Now we know on which rows to operate. We then check whether a medium that wasn’t used in the first wave only got a 3 or a 6 on the estimate in waves 2 to 6 and flag the person by medium combination that indeed doesn’t have a change. Afterwards, we transform those no_change indicators to the wide format. Below shows how these data look like: If it’s NA the person provided an estimate at the first wave. If it’s FALSE the person didn’t provide an estimate at wave 1 (so it was 0 minutes and hours), but their estimate increased or decreased over the next waves. If it’s TRUE the person didn’t provide an estimate at wave 1 and they kept not using or their use didn’t change. no_changes &lt;- media_long %&gt;% filter(selected == TRUE) %&gt;% # everyone who had an NA for an estimate in the first wave filter(wave &gt; 1) %&gt;% # we don&#39;t count the first wave group_by(id, medium) %&gt;% mutate(no_change = if_else(estimate %in% c(3,6), 0, 1)) %&gt;% # if it&#39;s 3 or 6, we assign zero summarise( # so that the sum is zero no_change = sum(no_change) ) %&gt;% mutate(no_change = if_else(no_change == 0, TRUE, FALSE)) %&gt;% # then anything that isn&#39;t zero has at least one 1,2, or 4 in their estimate ungroup() %&gt;% # then turn to wide format pivot_wider( ., names_from = &quot;medium&quot;, values_from = &quot;no_change&quot; ) %&gt;% rename_with( ., .cols = -id, ~ paste0(., &quot;_no_change&quot;) ) ## `summarise()` regrouping output by &#39;id&#39; (override with `.groups` argument) knitr::kable(head(no_changes, n = 10)) id audiobooks_no_change games_no_change tv_no_change books_no_change magazines_no_change music_no_change films_no_change pp_1 FALSE FALSE NA NA NA NA NA pp_10 TRUE NA FALSE NA NA NA NA pp_100 TRUE NA FALSE FALSE NA NA NA pp_1000 TRUE TRUE NA NA NA NA NA pp_1001 TRUE TRUE NA FALSE TRUE TRUE NA pp_1002 FALSE NA FALSE NA NA NA NA pp_1003 TRUE NA NA NA NA NA NA pp_1004 TRUE NA TRUE TRUE NA NA NA pp_1005 FALSE NA NA NA NA NA NA pp_1006 TRUE FALSE NA NA TRUE NA NA Now we can add those markers to the working_file. working_file &lt;- left_join( working_file, no_changes, by = c(&quot;id&quot;) ) Last, we transform the minutes and hours for each medium in each wave to 0, depending on whether that medium has one of the above markers telling us that the person didn’t use the medium at wave 1 and use didn’t change in subsequent waves (i.e., “not changed” or “haven’t used” on estimate for all subsequent waves). working_file &lt;- working_file %&gt;% mutate( # music across( c(music_hours, music_minutes), ~ if_else((wave != 1 &amp; music_no_change == TRUE), 0, .x, missing = .x) # need to set missing explicitly because the no_change variables contain NA if someone didn&#39;t have a missing in the first wave ), # films across( c(films_hours, films_minutes), ~ if_else((wave != 1 &amp; films_no_change == TRUE), 0, .x, missing = .x) ), # tv across( c(tv_hours, tv_minutes), ~ if_else((wave != 1 &amp; tv_no_change == TRUE), 0, .x, missing = .x) ), # video games across( c(games_hours, games_minutes), ~ if_else((wave != 1 &amp; games_no_change == TRUE), 0, .x, missing = .x) ), # ebooks across( c(books_hours, books_minutes), ~ if_else((wave != 1 &amp; books_no_change == TRUE), 0, .x, missing = .x) ), # magazines across( c(magazines_hours, magazines_minutes), ~ if_else((wave != 1 &amp; magazines_no_change == TRUE), 0, .x, missing = .x) ), # music across( c(audiobooks_hours, audiobooks_minutes), ~ if_else((wave != 1 &amp; audiobooks_no_change == TRUE), 0, .x, missing = .x) ) ) %&gt;% # no need anymore for the variables we added select( -ends_with(&quot;no_change&quot;) ) Let’s do a sanity check: all minutes and hours items should now have entries because they were forced response (or because we set them to zero). That’s indeed the case, so all looking good. working_file %&gt;% summarise( across( c(contains(&quot;minutes&quot;), contains(&quot;hours&quot;)), ~ sum(is.na(.x)) ) ) %&gt;% gather() ## # A tibble: 14 x 2 ## key value ## &lt;chr&gt; &lt;int&gt; ## 1 music_minutes 0 ## 2 films_minutes 0 ## 3 tv_minutes 0 ## 4 games_minutes 0 ## 5 books_minutes 0 ## 6 magazines_minutes 0 ## 7 audiobooks_minutes 0 ## 8 music_hours 0 ## 9 films_hours 0 ## 10 tv_hours 0 ## 11 games_hours 0 ## 12 books_hours 0 ## 13 magazines_hours 0 ## 14 audiobooks_hours 0 After all this wrangling we’re finally able to look at how many responses we have per medium per wave. Actually, the answer is straightforward now: everyone who finished a wave either provided minutes and hours estimates for all media, or they didn’t which means we set them to 0. In other words, the complete responses per wave are also the complete responses per medium. "],
["descriptives-and-visualizations.html", "3 Descriptives and visualizations", " 3 Descriptives and visualizations This is where descriptive info and visualizations will go. "]
]
